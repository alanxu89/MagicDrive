# MagicDrive multi-view video generation with first-person view depth and semantic_map inputs

This repo is based on the original [MagicDrive3D](https://github.com/flymin/MagicDrive3D) on **3D scene generation**, but extends it to video, and uses first-person view depth map and semantic map as control conditions. We have made a lot of modifications to the original architecture in order to generate consistent multi-view video give input conditions, including:
- nuScenes data processing pipeline to generate [depth map](tools/fpv_nuscenes/process_depth.py) using Depth-Anything-V2 and [fpv semantic map](tools/fpv_nuscenes/process_semantic_map.py) with the nuScenes map api for each camera and for each frame in a scene. We then iterate over the available scenes and collect relevant information to a [csv file](tools/fpv_nuscenes/process_frames.py) for further data processing. 
- We then design a [DatasetFromCSV](magicdrive/dataset/fpv_nuscenes_dataset.py) to load scene data into pytorch tensors, including rgb, depth, semantic, text description, etc. 
- There is also a newly designed [fpv_runner](magicdrive/runner/fpv_runner.py) where we wrap a MultiControlNet and stable diffusion Unet into a model, and load the 6D data (b, c, f, n, h, w) into the the model for diffusion training, with f as the data frames and n as the camera views.
- Model architucture changes: The controlnet in the original MagicDrive3D is being replaced by a MultiControlNet defined in the diffusers library, where it accepts depth and semantic_map as conditional inputs. The overall structure of the [Unet](magicdrive/networks/unet_2d_condition_fpv.py) remains unchanged, but all the BasicTransformerBlock are being replaced by the [BasicMultiviewVideoTransformerBlock](magicdrive/networks/blocks_video.py#L36). This new block adds three types of attentions: (1) [SparseCausalAttention](magicdrive/networks/blocks_video.py#L378) between frame i and (frame 0 and frame i-1), (2) cross-view attention between neighboring camera views, and (3) temporal attention along the frame axis (same h and w) of video data.
- A new pipeline for video generation with depth and map inputs, as detailed in [pipeline_fpv_controlnet](magicdrive/pipeline/pipeline_fpv_controlnet.py).

The default branch is now the alanxu/fpv branch where all the changes are made, while the main branch is a direct copy of the [MagicDrive3D](https://github.com/flymin/MagicDrive3D) repo without any modifications. 